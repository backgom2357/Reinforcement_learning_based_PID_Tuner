{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Lambda, Input\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ivmech PID Controller is simple implementation of a Proportional-Integral-Derivative (PID) Controller in the Python Programming Language.\n",
    "More information about PID Controller: http://en.wikipedia.org/wiki/PID_controller\n",
    "\"\"\"\n",
    "\n",
    "class PID:\n",
    "    \"\"\"PID Controller\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, P=0.2, I=0.0, D=0.0, current_time=None):\n",
    "\n",
    "        self.Kp = P\n",
    "        self.Ki = I\n",
    "        self.Kd = D\n",
    "\n",
    "        self.sample_time = 0.00\n",
    "        self.current_time = current_time if current_time is not None else time.time()\n",
    "        self.last_time = self.current_time\n",
    "\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clears PID computations and coefficients\"\"\"\n",
    "        self.SetPoint = np.array([1, 0])\n",
    "\n",
    "        self.PTerm = np.zeros(2)\n",
    "        self.ITerm = np.zeros(2)\n",
    "        self.DTerm = np.zeros(2)\n",
    "        self.last_error = np.zeros(2)\n",
    "\n",
    "        # Windup Guard\n",
    "        self.int_error = np.zeros(2)\n",
    "        self.windup_guard = np.array([20.0])\n",
    "\n",
    "        self.output = np.zeros(2)\n",
    "\n",
    "    def update(self, feedback_value, current_time=None):\n",
    "        \"\"\"Calculates PID value for given reference feedback\n",
    "\n",
    "        .. math::\n",
    "            u(t) = K_p e(t) + K_i \\int_{0}^{t} e(t)dt + K_d {de}/{dt}\n",
    "\n",
    "        .. figure:: images/pid_1.png\n",
    "           :align:   center\n",
    "\n",
    "           Test PID with Kp=1.2, Ki=1, Kd=0.001 (test_pid.py)\n",
    "\n",
    "        \"\"\"\n",
    "        error = self.SetPoint - feedback_value\n",
    "\n",
    "        self.current_time = current_time if current_time is not None else time.time()\n",
    "        delta_time = self.current_time - self.last_time\n",
    "        delta_error = error - self.last_error\n",
    "\n",
    "        if (delta_time >= self.sample_time):\n",
    "            self.PTerm = self.Kp * error\n",
    "            self.ITerm += error * delta_time\n",
    "\n",
    "#             if (self.ITerm < -self.windup_guard):\n",
    "#                 self.ITerm = -self.windup_guard\n",
    "#             elif (self.ITerm > self.windup_guard):\n",
    "#                 self.ITerm = self.windup_guard\n",
    "\n",
    "            self.DTerm = 0.0\n",
    "            if delta_time > 0:\n",
    "                self.DTerm = delta_error / delta_time\n",
    "\n",
    "            # Remember last time and last error for next calculation\n",
    "            self.last_time = self.current_time\n",
    "            self.last_error = error\n",
    "\n",
    "            self.output = self.PTerm + (self.Ki * self.ITerm) + (self.Kd * self.DTerm)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def setKs(self, proportional_gain, integral_gain, derivative_gain):\n",
    "        \"\"\"Determines how aggressively the PID reacts to the current error with setting Proportional Gain, Integral Gain and Derivative Gain\"\"\"\n",
    "        self.Kp = proportional_gain\n",
    "        self.Ki = integral_gain\n",
    "        self.Kd = derivative_gain        \n",
    "\n",
    "    def setWindup(self, windup):\n",
    "        \"\"\"Integral windup, also known as integrator windup or reset windup,\n",
    "        refers to the situation in a PID feedback controller where\n",
    "        a large change in setpoint occurs (say a positive change)\n",
    "        and the integral terms accumulates a significant error\n",
    "        during the rise (windup), thus overshooting and continuing\n",
    "        to increase as this accumulated error is unwound\n",
    "        (offset by errors in the other direction).\n",
    "        The specific problem is the excess overshooting.\n",
    "        \"\"\"\n",
    "        self.windup_guard = windup\n",
    "\n",
    "    def setSampleTime(self, sample_time):\n",
    "        \"\"\"PID that should be updated at a regular interval.\n",
    "        Based on a pre-determined sampe time, the PID decides if it should compute or return immediately.\n",
    "        \"\"\"\n",
    "        self.sample_time = sample_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    \"\"\"\n",
    "        PPO Actor Neural Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, pid, pid_parameters, action_bound, learning_rate, ratio_clipping):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.pid_parameters = pid_parameters\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ratio_clipping = ratio_clipping\n",
    "\n",
    "        # set min and max of standard deviation\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        # create actor neural net\n",
    "        self.model, self.theta = self.build_network()\n",
    "        \n",
    "        # pid\n",
    "        self.pid = pid\n",
    "\n",
    "        \"\"\"\n",
    "        tf 2.0\n",
    "        \"\"\"\n",
    "\n",
    "        # loss and optimizer\n",
    "        self.actor_optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        \n",
    "        # pid_parameters : 3\n",
    "        mu = Dense(self.pid_parameters, activation='tanh')(h3)\n",
    "        std = Dense(self.pid_parameters, activation='softplus')(h3)\n",
    "        \n",
    "        model = Model(state_input, [mu, std])\n",
    "        return model, model.trainable_weights\n",
    "\n",
    "    def train(self, states, actions, advantages, log_old_policy_pdf):\n",
    "        with tf.GradientTape() as g:\n",
    "            mu, std = self.model(states)\n",
    "            log_policy_pdf = self.log_pdf(mu, std , actions)\n",
    "\n",
    "            # ratio of two policy & target function surrogate\n",
    "            ratio = tf.exp(log_policy_pdf - log_old_policy_pdf)\n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1.0-self.ratio_clipping, 1.0 + self.ratio_clipping)\n",
    "            surrogate = -tf.minimum(ratio * advantages, clipped_ratio * advantages)\n",
    "            loss = tf.reduce_mean(surrogate)\n",
    "        dj_dtheta = g.gradient(loss, self.theta)\n",
    "        grads = zip(dj_dtheta, self.theta)\n",
    "        self.actor_optimizer.apply_gradients(grads)\n",
    "\n",
    "    # log_policy pdf\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std**2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    # get agent's action\n",
    "    def get_policy_action(self, state):\n",
    "        mu, std = self.model(np.reshape(state, [1, self.state_dim]))\n",
    "        mu, std = mu[0], std[0]\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        \n",
    "        p,i,d = np.random.normal(mu, std)\n",
    "        action = self.get_action_from_pid(p, i, d, state)\n",
    "        \n",
    "        return mu, std, action\n",
    "    \n",
    "    # get action from pid\n",
    "    def get_action_from_pid(self, p, i, d, state):\n",
    "        self.pid.setKs(p,i,d)\n",
    "        val = self.pid.update(np.array(state[:1]))\n",
    "        action = val[1]/(val[0] + 0.000001)\n",
    "        return [action]\n",
    "\n",
    "    # calculate mean\n",
    "    def predict(self, state):\n",
    "        mu, _ = self.model(np.reshape(state, [1, self.state_dim]))\n",
    "        action = self.get_action_from_pid(*mu, state)\n",
    "        return action\n",
    "\n",
    "    # save Actor parameter\n",
    "    def save_weights(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    # load Actor parameter\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path+'pendulum_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\"\"\"\n",
    "    TF2.2 functional API\n",
    "\"\"\"\n",
    "\n",
    "class Critic(object):\n",
    "\n",
    "    \"\"\"\n",
    "        PPO Critic Neural Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, learning_rate):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # create critic neural net\n",
    "        self.model = self.build_network()\n",
    "\n",
    "        # set training method\n",
    "        self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
    "\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        v_output = Dense(1, activation='linear')(h3)\n",
    "\n",
    "        model = Model(state_input, v_output)\n",
    "        return model\n",
    "\n",
    "    # update neural net with batch data\n",
    "    def train_on_batch(self, states, td_targets):\n",
    "        return self.model.train_on_batch(states, td_targets)\n",
    "\n",
    "    # save Critic parameter\n",
    "    def save_weights(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    # load Critic parameter\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path+'pendulum_critic.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOagent(object):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        # hyperparameter\n",
    "        self.GAMMA = 0.95\n",
    "        self.GAE_LAMBDA = 0.9\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.RATIO_CLIPPING = 0.2\n",
    "        self.EPOCHS = 10\n",
    "        self.t_MAX = 4\n",
    "\n",
    "        # environment\n",
    "        self.env = env\n",
    "        # state dimension\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        # action dimension\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        # max action size\n",
    "        self.action_bound = env.action_space.high[0]\n",
    "        \n",
    "        # PID\n",
    "        self.pid = PID()\n",
    "        self.pid_parameters = 3\n",
    "\n",
    "        # create Actor and Critic neural nets\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.pid, self.pid_parameters, \n",
    "                           self.action_bound, self.ACTOR_LEARNING_RATE, self.RATIO_CLIPPING)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # total reward of a episode\n",
    "        self.save_epi_reward = []\n",
    "\n",
    "    # calculate GAE and TD targets\n",
    "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
    "        n_stop_targets = np.zeros_like(rewards)\n",
    "        gae = np.zeros_like(rewards)\n",
    "        gae_cumulative = 0\n",
    "        forward_val = 0\n",
    "\n",
    "        if not done:\n",
    "            forward_val = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            delta = rewards[k] + self.GAMMA * forward_val - v_values[k]\n",
    "            gae_cumulative = self.GAMMA * self.GAE_LAMBDA * gae_cumulative + delta\n",
    "            gae[k] = gae_cumulative\n",
    "            forward_val = v_values[k]\n",
    "            n_stop_targets[k] = gae[k] + v_values[k]\n",
    "        return gae, n_stop_targets\n",
    "\n",
    "    # train agent\n",
    "    def train(self, max_episode_num):\n",
    "\n",
    "        # repeat for each episode\n",
    "        for ep in range(int(max_episode_num)):\n",
    "\n",
    "            # init states, actions, reward\n",
    "            states, actions, rewards = [], [], []\n",
    "            # init log old policy pdf\n",
    "            log_old_policy_pdfs = []\n",
    "            # init episode\n",
    "            time, episode_reward, done = 0, 0, False\n",
    "            # reset env and observe initial state\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                # visualize env\n",
    "                self.env.render()\n",
    "\n",
    "                # get action\n",
    "                mu_old, std_old, action = self.actor.get_policy_action(state)\n",
    "\n",
    "                # bound action range\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "\n",
    "                # calculate log old policy pdf\n",
    "                var_old = std_old**2\n",
    "                log_old_policy_pdf = -0.5 * (action - mu_old)**2 / var_old - 0.5 * np.log(var_old * 2 * np.pi)\n",
    "                log_old_policy_pdf = np.sum(log_old_policy_pdf)\n",
    "\n",
    "                # observe next state, reward\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # save to batch\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append((reward + 8) / 8)  # modify reward range\n",
    "                log_old_policy_pdfs.append(log_old_policy_pdf)\n",
    "\n",
    "                # state update\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                time += 1\n",
    "\n",
    "                if len(states) == self.t_MAX or done:\n",
    "\n",
    "                    # get data from batch\n",
    "                    states = np.array(states)\n",
    "                    actions = np.array(actions)\n",
    "                    rewards = np.array(rewards)\n",
    "                    log_old_policy_pdfs = np.array(log_old_policy_pdfs)\n",
    "\n",
    "\n",
    "                    # calculate n-step TD target and advantage\n",
    "                    next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                    next_v_value = self.critic.model(next_state)\n",
    "                    v_values = self.critic.model(states)\n",
    "                    gaes, y_i = self.gae_target(rewards, v_values, next_v_value, done)\n",
    "\n",
    "                    for _ in range(self.EPOCHS):\n",
    "                        # train actor network\n",
    "                        self.actor.train(states, actions, gaes, log_old_policy_pdfs)\n",
    "                        # train critic network\n",
    "                        self.critic.train_on_batch(states, y_i)\n",
    "\n",
    "                    # clear batch\n",
    "                    states, actions, rewards = [], [], []\n",
    "                    log_old_policy_pdfs = []\n",
    "\n",
    "            print(\"Epi: \", ep+1, \"Time: \", time, \"Reward: \", episode_reward, \"PID : \", self.actor.pid.Kp,self.actor.pid.Ki,self.actor.pid.Kd)\n",
    "            self.save_epi_reward.append(episode_reward)\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                self.actor.save_weights(\"./save_weights/pendulum_actor.h5\")\n",
    "                self.critic.save_weights(\"./save_weights/pendulum_actor.h5\")\n",
    "\n",
    "        np.savetxt('.save_weights/pendulum_epi_reward.txt', self.save_epi_reward)\n",
    "        print(self.save_epi_reward)\n",
    "\n",
    "    # graph episodes and rewards\n",
    "    def plot_result(self):\n",
    "        plt.plot(self.save_epi_reward)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    max_episode_num = 1000\n",
    "    env_name = 'Pendulum-v0'\n",
    "    env = gym.make(env_name)\n",
    "    agent = PPOagent(env)\n",
    "    \n",
    "    # train\n",
    "    agent.train(max_episode_num)\n",
    "    \n",
    "    # visualization\n",
    "    agent.plot_result()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     env_name = \"Pendulum-v0\"\n",
    "#     env = gym.make(env_name)\n",
    "#     agent = PPOagent(env_name)\n",
    "\n",
    "#     agent.actor.load_weights('./save_weights/')\n",
    "#     agent.critic.load_weights('./save_weights/')\n",
    "\n",
    "#     time = 0\n",
    "\n",
    "#     state = env.reset()\n",
    "\n",
    "#     while True:\n",
    "#         env.render()\n",
    "#         action = agent.global_actor.predict(state)\n",
    "#         state, reward, done, _ = env.step(action)\n",
    "#         time += 1\n",
    "\n",
    "#         print(\"Time: {}, Reward: {}\".format(time, reward))\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     env.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
